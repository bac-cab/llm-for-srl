{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rexhaif/miniforge3/envs/llm-srl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "%load_ext rich\n",
    "import datasets as ds\n",
    "from rich import print\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('groupped_examples.json', 'r') as f:\n",
    "    examples = json.loads(f.read())\n",
    "\n",
    "with open(\"role-mapping.json\", 'r') as f:\n",
    "    role_mapping = json.loads(f.read())\n",
    "\n",
    "with open(\"form-mapping.json\", 'r') as f:\n",
    "    form_mapping = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "model = spacy.load(\"ru_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatize(word):\n",
    "    return next(iter(model(word))).lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_form_mapping = {}\n",
    "for group, forms in form_mapping.items():\n",
    "    lemmas = [spacy_lemmatize(f) for f in forms]\n",
    "    for lemma in lemmas:\n",
    "        inv_form_mapping[lemma] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_examples_mapping = {}\n",
    "for ex in examples:\n",
    "    group = ex['group']\n",
    "    if group not in inv_examples_mapping:\n",
    "        inv_examples_mapping[group] = []\n",
    "\n",
    "    inv_examples_mapping[group].append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds.Dataset.from_csv(\"./data/KP_robotics_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remapper(example):\n",
    "    return {\n",
    "        'text_fixed': \" \".join(example['tokens']) if 'tokens' in example else example['text'] if 'text' in example else example['text_fixed']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(remapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_verbs(text):\n",
    "    verbs, forms = list(), list()\n",
    "    for tok in model(text):\n",
    "        if tok.pos_ == \"VERB\":\n",
    "            verbs.append(tok.lemma_)\n",
    "            forms.append(tok.text)\n",
    "    return verbs, forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lemmas = set(inv_form_mapping.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets as ds\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fn(example):\n",
    "    return len(set(find_verbs(example['text_fixed'])[0]).intersection(target_lemmas)) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty text_fixed\n",
    "data = data.filter(lambda x: x['text_fixed'] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only examples with target verbs\n",
    "data = data.filter(filter_fn, num_proc=16, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6984/6984 [02:11<00:00, 53.06it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6984</span> =&gt; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50842</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m6984\u001b[0m => \u001b[1;36m50842\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split into sentences\n",
    "data_list = data.to_list()\n",
    "new_data = []\n",
    "for item in tqdm(data_list):\n",
    "    item_desc = item.copy()\n",
    "    text = item_desc.pop(\"text_fixed\")\n",
    "    for sent in model(text).sents:\n",
    "        item_desc_i = item_desc.copy()\n",
    "        item_desc_i['text_fixed'] = sent.text\n",
    "        new_data.append(item_desc_i)\n",
    "print(f\"{len(data)} => {len(new_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=16): 100%|██████████| 50842/50842 [00:29<00:00, 1711.51 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m7602\u001b[0m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter again to keep only sentences with target verbs\n",
    "data = ds.Dataset.from_list(new_data)\n",
    "data = data.filter(filter_fn, num_proc=16, batched=False)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicate_extractor(text):\n",
    "    lemmas, forms = find_verbs(text)\n",
    "    new_lemmas, new_forms = list(), list()\n",
    "    for lemma, form in zip(lemmas, forms):\n",
    "        if lemma in set(inv_form_mapping.keys()):\n",
    "            new_lemmas.append(lemma)\n",
    "            new_forms.append(form)\n",
    "    return {\n",
    "        'predicate': new_forms,\n",
    "        'lemma': new_lemmas\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicate_extractor_fn(example):\n",
    "    return predicate_extractor(example['text_fixed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7602/7602 [00:35<00:00, 214.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# extract predicates to identify relevant few-shot examples\n",
    "data = data.map(predicate_extractor_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_pred_group(example):\n",
    "    group = None\n",
    "    for lemma in example['lemma']:\n",
    "        if lemma in inv_form_mapping:\n",
    "            group = inv_form_mapping[lemma]\n",
    "    return {\n",
    "        'predicate_group': group\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/7602 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7602/7602 [00:00<00:00, 48752.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data = data.map(map_to_pred_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal\n",
    "from pydantic import BaseModel, Field, model_validator\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "class SemanticRole(BaseModel):\n",
    "    short_reasoning: Annotated[str, Field(min_length=16, max_length=64)]\n",
    "    arg_role: Literal[\"Cause\", \"Experiencer\", \"Causator\", \"Deliberative\", \"Instrument\", \"Object\", \"Not-Applicable\"]\n",
    "    arg_phrase_or_clause: Annotated[str, Field(min_length=1, max_length=64)]\n",
    "    arg_main_indicative_word: Annotated[str, Field(min_length=1, max_length=32)]\n",
    "\n",
    "class SemanticRoleMarkup(BaseModel):\n",
    "    roles: List[SemanticRole]\n",
    "    model_config = {\n",
    "        \"title\": \"SemanticRoleMarkup\",\n",
    "        \"description\": \"Semantic Role Markup\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt_for_example_vllm(example):\n",
    "    example_predicate_group = example['predicate_group']\n",
    "    rule_set = role_mapping[example_predicate_group]\n",
    "    rule_set = json.dumps(rule_set, ensure_ascii=False, indent=4)\n",
    "    example_set = inv_examples_mapping[example_predicate_group]\n",
    "\n",
    "    prompt = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': f'You are native russian linguist specializing in semantic role labelling.\\nYou must follow those rules in your work:\\n\\n```json{rule_set}```'\n",
    "        }\n",
    "    ]\n",
    "    examples = \"\"\n",
    "    for ex in example_set:\n",
    "        answers = [answer['entity'] for answer in ex['roles'] if '#predicate' not in answer['entity']]\n",
    "        text = ex['text']\n",
    "        semantic_roles = \"\".join(f\"- {ans}\\n\" for ans in answers)\n",
    "        examples += f\"Example Text: {ex['text']}\\n\"\n",
    "        examples += f\"Example Semantic Roles: {semantic_roles}\\n\\n\"\n",
    "\n",
    "    inputs = f\"\"\"\n",
    "Given a series of few-shot examples, please predict semantic roles in a target example.\n",
    "Here are the few-shot examples:\n",
    "{examples}\n",
    "\n",
    "Here is the target example:\n",
    "{example['text_fixed']}\n",
    "\n",
    "Instructions:\n",
    "- Do not mark semantic roles for implied, implicit or otherwise not presented arguments\n",
    "- Reason out loud (concisely) before answering\n",
    "- Predict both argument phrase (or clause) and a main indicative word of such phrase\n",
    "- Some arguments may not have a phrase and will be represented by a single word. In this case use it as both argument phrase and main indicative word\n",
    "\n",
    "Important: If there are no semantic roles for any argument that you can extract - reply with a ONLY ONE SINGLE argument markup that will have a role \"Not-Applicable\".\n",
    "\"\"\".strip()\n",
    "\n",
    "    \n",
    "\n",
    "    prompt.append({\n",
    "        'role': 'user',\n",
    "        'content': inputs\n",
    "    })\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt_openai_for_example(example):\n",
    "    example_predicate_group = example['predicate_group']\n",
    "    rule_set = role_mapping[example_predicate_group]\n",
    "    rule_set = json.dumps(rule_set, ensure_ascii=False, indent=4)\n",
    "    example_set = inv_examples_mapping[example_predicate_group]\n",
    "\n",
    "    prompt = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': f'You are native russian linguist specializing in semantic role labelling.If there are no roles: reply with - No-Roles#No-Roles.\\nYou must follow those rules in your work:\\n\\n```json{rule_set}```'\n",
    "        }\n",
    "    ]\n",
    "    for ex in example_set:\n",
    "        answers = [answer['entity'] for answer in ex['roles'] if '#predicate' not in answer['entity']]\n",
    "        text = ex['text']\n",
    "        semantic_roles = \"\".join(f\"- {ans}\\n\" for ans in answers)\n",
    "        prompt.append({\n",
    "            'role': 'user',\n",
    "            'content': text\n",
    "        })\n",
    "        prompt.append({\n",
    "            'role': 'assistant',\n",
    "            'content': semantic_roles\n",
    "        })\n",
    "        \n",
    "\n",
    "    prompt.append({\n",
    "        'role': 'user',\n",
    "        'content': example['text_fixed']\n",
    "    })\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request_openai(example):\n",
    "    messages = make_prompt_openai_for_example(example)\n",
    "    client = OpenAI(\n",
    "        api_key=\"\",\n",
    "        base_url=\"\"\n",
    "    )\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model='openai/gpt-4o', messages=messages, max_tokens=128, temperature=0.0,\n",
    "        )\n",
    "        return {\n",
    "            'llm-response': response.choices[0].message.content\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'llm-response': f\"ERROR + {e}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request_vllm(example):\n",
    "    messages = make_prompt_for_example_vllm(example)\n",
    "    client = OpenAI(\n",
    "        base_url=\"http://localhost:8000/v1\",\n",
    "        api_key=\"token-abc123\"\n",
    "    )\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model='t-tech/T-lite-it-1.0',\n",
    "            messages=messages, \n",
    "            max_completion_tokens=1024,\n",
    "            temperature=0.0,\n",
    "            extra_body={\n",
    "                \"guided_json\": SemanticRoleMarkup.model_json_schema()\n",
    "            },\n",
    "        )\n",
    "        return {\n",
    "            'llm-response': response.choices[0].message.content\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'llm-response': f\"ERROR + {e}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">- Вася#Experiencer\n",
       "- брата#Deliberative\n",
       "</pre>\n"
      ],
      "text/plain": [
       "- Вася#Experiencer\n",
       "- брата#Deliberative\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(make_request_openai({'text_fixed': \"Вася боиться за брата\", \"predicate_group\": \"бояться\"})['llm-response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|██████████| 7602/7602 [09:45<00:00, 12.98 examples/s]  \n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "data = data.map(make_request_openai, num_proc=16)\n",
    "t2 = time.time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_roles_vllm(example):\n",
    "    if example['llm-response'] is not None:\n",
    "        roles = []\n",
    "        try:\n",
    "            response = json.loads(example['llm-response'])\n",
    "            for item in response['roles']:\n",
    "                #print(item)\n",
    "                role = item['arg_role']\n",
    "                content = item['arg_phrase_or_clause']\n",
    "                if 'Not-Applicable' not in {role, content}:\n",
    "                    roles.append({\n",
    "                        'role': role.strip(),\n",
    "                        'argument': content\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        return {'roles': roles}\n",
    "    else:\n",
    "        return {'roles': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_roles_openai(example):\n",
    "    if example['llm-response'] is not None and '- No-Roles#No-Roles' not in example['llm-response'] and \"ERROR +\" not in example['llm-response']:\n",
    "        roles = []\n",
    "        for item in example['llm-response'].split(\"\\n\"):\n",
    "            role = item.split(\"#\")[-1]\n",
    "            argument = item.replace(f\"#{role}\", \"\")\n",
    "            argument = argument.replace(\"- \", \"\")\n",
    "            roles.append({\n",
    "                'argument': argument,\n",
    "                'role': role\n",
    "            })\n",
    "        return {'roles': roles}\n",
    "    else:\n",
    "        return {'roles': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7602/7602 [00:00<00:00, 28534.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data = data.map(fix_roles_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'global_id'\u001b[0m: \u001b[32m'-170528132_4268704_4268822_vk'\u001b[0m,\n",
       "    \u001b[32m'text'\u001b[0m: \u001b[32m'Почти как собачка! А боится машин дёргается видно страшно!'\u001b[0m,\n",
       "    \u001b[32m'text_fixed'\u001b[0m: \u001b[32m'А боится машин дёргается видно страшно!'\u001b[0m,\n",
       "    \u001b[32m'predicate'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'боится'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'lemma'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'бояться'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'predicate_group'\u001b[0m: \u001b[32m'бояться'\u001b[0m,\n",
       "    \u001b[32m'llm-response'\u001b[0m: \u001b[32m'- \u001b[0m\u001b[32m[\u001b[0m\u001b[32mбоится машин\u001b[0m\u001b[32m]\u001b[0m\u001b[32m@\u001b[0m\u001b[32m[\u001b[0m\u001b[32mмашин\u001b[0m\u001b[32m]\u001b[0m\u001b[32m#Causator'\u001b[0m,\n",
       "    \u001b[32m'roles'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'argument'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mбоится машин\u001b[0m\u001b[32m]\u001b[0m\u001b[32m@\u001b[0m\u001b[32m[\u001b[0m\u001b[32mмашин\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'role'\u001b[0m: \u001b[32m'Causator'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def create_roles_dataframe(examples: List[Dict]) -> pd.DataFrame:\n",
    "    # List to store all rows\n",
    "    rows = []\n",
    "    \n",
    "    for example in tqdm(examples):\n",
    "        # Get roles list from the example\n",
    "        roles = example.get('roles', [])\n",
    "        \n",
    "        # For each role in the example (except No-Roles)\n",
    "        for role_dict in roles:\n",
    "            if role_dict['role'] == 'No-Roles':\n",
    "                continue\n",
    "                \n",
    "            # Create a new row with all metadata and role information\n",
    "            row = {\n",
    "                'group': example.get('group'),\n",
    "                'global_id': example.get('global_id'),\n",
    "                'date': example.get('date'),\n",
    "                'text': example.get('text_fixed'),\n",
    "                'predicate': example.get('predicate', [''])[0],  # Take first predicate\n",
    "                'lemma': example.get('lemma', [''])[0],  # Take first lemma\n",
    "                'predicate_group': example.get('predicate_group'),\n",
    "                'llm_response': example.get('llm-response'),\n",
    "                'has_negation': example.get('has_negation'),\n",
    "                'argument': role_dict.get('argument'),\n",
    "                'role': role_dict.get('role')\n",
    "            }\n",
    "            rows.append(row)\n",
    "    \n",
    "    # Create DataFrame from all rows\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7602 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7602/7602 [00:00<00:00, 29381.39it/s]\n"
     ]
    }
   ],
   "source": [
    "data_frame = create_roles_dataframe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame.drop(['llm_response'], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
